{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#from plotly import plotly\n",
    "#import plotly.offline as offline\n",
    "#import plotly.graph_objs as go\n",
    "#offline.init_notebook_mode()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "\n",
       "  clean_categories                 clean_subcategories  \\\n",
       "0     math_science  appliedsciences health_lifescience   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data  = pd.read_csv('preprocessed_data.csv', nrows=5000)\n",
    "# data  = pd.read_csv('preprocessed_data.csv', nrows=50000) # you can take less number of rows like this\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorizing Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_essays = data['essay'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (5000, 4557)\n"
     ]
    }
   ],
   "source": [
    "# We are considering only the words which appeared in at least 10 documents(rows or projects).\n",
    "vectorizer = CountVectorizer(min_df=10)\n",
    "text_bow = vectorizer.fit_transform(preprocessed_essays)\n",
    "print(\"Shape of matrix after one hot encodig \",text_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can vectorize the title also \n",
    "# before you vectorize the title make sure you preprocess it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (5000, 4557)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10)\n",
    "text_tfidf = vectorizer.fit_transform(preprocessed_essays)\n",
    "print(\"Shape of matrix after one hot encodig \",text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using Pretrained Models: Avg W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\\ndef loadGloveModel(gloveFile):\\n    print (\"Loading Glove Model\")\\n    f = open(gloveFile,\\'r\\', encoding=\"utf8\")\\n    model = {}\\n    for line in tqdm(f):\\n        splitLine = line.split()\\n        word = splitLine[0]\\n        embedding = np.array([float(val) for val in splitLine[1:]])\\n        model[word] = embedding\\n    print (\"Done.\",len(model),\" words loaded!\")\\n    return model\\nmodel = loadGloveModel(\\'glove.42B.300d.txt\\')\\n\\n# ============================\\nOutput:\\n    \\nLoading Glove Model\\n1917495it [06:32, 4879.69it/s]\\nDone. 1917495  words loaded!\\n\\n# ============================\\n\\nwords = []\\nfor i in preproced_texts:\\n    words.extend(i.split(\\' \\'))\\n\\nfor i in preproced_titles:\\n    words.extend(i.split(\\' \\'))\\nprint(\"all the words in the coupus\", len(words))\\nwords = set(words)\\nprint(\"the unique words in the coupus\", len(words))\\n\\ninter_words = set(model.keys()).intersection(words)\\nprint(\"The number of words that are present in both glove vectors and our coupus\",       len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\\n\\nwords_courpus = {}\\nwords_glove = set(model.keys())\\nfor i in words:\\n    if i in words_glove:\\n        words_courpus[i] = model[i]\\nprint(\"word 2 vec length\", len(words_courpus))\\n\\n\\n# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\\n\\nimport pickle\\nwith open(\\'glove_vectors\\', \\'wb\\') as f:\\n    pickle.dump(words_courpus, f)\\n\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = loadGloveModel('glove.42B.300d.txt')\n",
    "\n",
    "# ============================\n",
    "Output:\n",
    "    \n",
    "Loading Glove Model\n",
    "1917495it [06:32, 4879.69it/s]\n",
    "Done. 1917495  words loaded!\n",
    "\n",
    "# ============================\n",
    "\n",
    "words = []\n",
    "for i in preproced_texts:\n",
    "    words.extend(i.split(' '))\n",
    "\n",
    "for i in preproced_titles:\n",
    "    words.extend(i.split(' '))\n",
    "print(\"all the words in the coupus\", len(words))\n",
    "words = set(words)\n",
    "print(\"the unique words in the coupus\", len(words))\n",
    "\n",
    "inter_words = set(model.keys()).intersection(words)\n",
    "print(\"The number of words that are present in both glove vectors and our coupus\", \\\n",
    "      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n",
    "\n",
    "words_courpus = {}\n",
    "words_glove = set(model.keys())\n",
    "for i in words:\n",
    "    if i in words_glove:\n",
    "        words_courpus[i] = model[i]\n",
    "print(\"word 2 vec length\", len(words_courpus))\n",
    "\n",
    "\n",
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "\n",
    "import pickle\n",
    "with open('glove_vectors', 'wb') as f:\n",
    "    pickle.dump(words_courpus, f)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "# make sure you have the glove_vectors file\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_essays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4f749cb8f9fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# compute average word2vec for each review.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mavg_w2v_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m# the avg-w2v for each sentence/review is stored in this list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_essays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# for each review/sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# as word vectors are of zero length\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcnt_words\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m# num of words with a valid vector in the sentence/review\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_essays' is not defined"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if word in glove_words:\n",
    "            vector += model[word]\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        vector /= cnt_words\n",
    "    avg_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(avg_w2v_vectors))\n",
    "print(len(avg_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Using Pretrained Models: TFIDF weighted W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(preprocessed_essays)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "tfidf_words = set(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 403.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(preprocessed_essays): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    tfidf_w2v_vectors.append(vector)\n",
    "\n",
    "print(len(tfidf_w2v_vectors))\n",
    "print(len(tfidf_w2v_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorizing Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (5000, 51)\n"
     ]
    }
   ],
   "source": [
    "# provided we did the cleaning\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "school_state_ohe = vectorizer.fit_transform(data['school_state'].values)\n",
    "print(\"Shape of matrix after one hot encodig \",school_state_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
